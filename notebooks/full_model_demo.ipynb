{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NUVzBhNeSpP"
      },
      "outputs": [],
      "source": [
        "%cd ../src/main\n",
        "# !pip install yfinance\n",
        "# !wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\n",
        "# !tar -xzvf ta-lib-0.4.0-src.tar.gz\n",
        "# %cd ta-lib\n",
        "# !./configure --prefix=/usr\n",
        "# !make\n",
        "# !make install\n",
        "# !pip install Ta-Lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_X0tOlZa6Yc",
        "outputId": "8e496cc5-ad4f-46b3-e7d0-208bf7aae41e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys, os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SCRIPT_DIR = os.path.dirname(os.path.abspath(\"Project_2_with_coint_refactor.ipynb\"))\n",
        "#if SCRIPT_DIR not in sys.path:\n",
        "if 'backend' not in os.listdir(SCRIPT_DIR):\n",
        "    sys.path.append(os.path.dirname(SCRIPT_DIR))\n",
        "\n",
        "from backend.technical_analysis import (\n",
        "    get_data_set\n",
        ")\n",
        "from backend.stock_dataset import (\n",
        "    StockDataset\n",
        ")\n",
        "from backend.reinforcement_learning import (\n",
        "    rlPolicy\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check if GPU is available, if not then use CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBzCQdBka6Yi"
      },
      "outputs": [],
      "source": [
        "#define device\n",
        "\n",
        "\n",
        "if torch.cuda.device_count() >= 1:\n",
        "    device = torch.device(f'cuda:{torch.cuda.device_count()-1}')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P2NDmOjOa6Yk"
      },
      "outputs": [],
      "source": [
        "#Calculate Utility based on policy Output\n",
        "#z: z from dataset\n",
        "#c: transaction cost\n",
        "def calcUtility(policyOutput, z, c=0.0001):\n",
        "  #with torch.no_grad():\n",
        "    discretize=policyOutput.detach()\n",
        "    discretize=(discretize>=0)*2-1\n",
        "    preAction=torch.cat([discretize[:,0:1], discretize[:, :-1]], dim=1)\n",
        "    #net income R\n",
        "    R=z*discretize-c*((discretize-preAction)!=0)\n",
        "    U=[]\n",
        "    for i in range(R.shape[1]):\n",
        "        if(i==0):\n",
        "            u=R[:,i:i+1]\n",
        "        else:\n",
        "            u=R[:,i:i+1]+U[i-1]\n",
        "        U.append(u)\n",
        "    U=torch.cat(U, dim=1)\n",
        "    return U, preAction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HVHG_bRUa6Yk"
      },
      "outputs": [],
      "source": [
        "#Prevent exploding gradient\n",
        "def grad_clipping(net: torch.nn.Module, theta): \n",
        "    \"\"\"Clip the gradient.\"\"\"\n",
        "    if isinstance(net, nn.Module):\n",
        "        params = [p for p in net.parameters() if p.requires_grad]\n",
        "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
        "    if norm > theta:\n",
        "        for param in params:\n",
        "            param.grad[:] *= theta / norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NHx3WV0Ta6Yl"
      },
      "outputs": [],
      "source": [
        "#model weight initialization\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
        "        torch.nn.init.xavier_normal_(m.weight)\n",
        "        m.bias.data.normal_(0.0,0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0h436dd-a6Yl"
      },
      "outputs": [],
      "source": [
        "#Loss function defined by paper\n",
        "def lossFunc(predP, y, policyOutput, z, device: torch.device):\n",
        "    #MSE\n",
        "    term1=nn.MSELoss()(predP, y)\n",
        "    #RL\n",
        "    U, preAction=calcUtility(policyOutput, z)\n",
        "    U_detach=U.detach()\n",
        "    actionProb=(torch.tensor(1).to(device)+policyOutput)/torch.tensor(2)\n",
        "    plusMinus=(preAction<0)*1\n",
        "    term2=-torch.log(1*plusMinus+((-1)**plusMinus)*actionProb)*U_detach\n",
        "    return term2.mean()+term1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RMgC-Ljmn50s"
      },
      "outputs": [],
      "source": [
        "#greedy loss function\n",
        "def lossFunc2(predP, y, policyOutput, z, device):\n",
        "    #MSE\n",
        "    term1=nn.MSELoss()(predP, y)\n",
        "    #RL\n",
        "    greedyAction=(z>=0.01)*2.0-1.0\n",
        "    U, preAction=calcUtility(policyOutput, z)\n",
        "    U_detach=U.detach()\n",
        "    actionProb=(torch.tensor(1).to(device)+policyOutput)/torch.tensor(2)\n",
        "    plusMinus=(preAction<0)*1\n",
        "    term2=(torch.log(1*plusMinus+((-1)**plusMinus)*actionProb)*U_detach).mean()\n",
        "    term3=nn.MSELoss()(policyOutput, greedyAction)\n",
        "    return term3+term2+term1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ly7ecW78a6Yl"
      },
      "outputs": [],
      "source": [
        "#trainer for epoch\n",
        "def train_epoch(net: torch.nn.Module, train_iter: DataLoader, device: torch.device, optimizer: torch.optim.Optimizer):\n",
        "    loss_data=[]\n",
        "    with torch.autograd.set_detect_anomaly(True):\n",
        "        for X, y, z, _ in train_iter:\n",
        "            #reset state for each batch\n",
        "            state=net.begin_state(batch_size=X.shape[0], device=device)\n",
        "        \n",
        "            #move to device\n",
        "            X, y, z=X.to(device), y.to(device), z.to(device)\n",
        "            predP, output=net(X, state)\n",
        "            loss=lossFunc2(predP, y, output,z, device)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            grad_clipping(net, 1)\n",
        "            optimizer.step()\n",
        "            loss_data.append(loss.item())\n",
        "    return np.array(loss_data).mean(), loss_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "39OBubURa6Ym"
      },
      "outputs": [],
      "source": [
        "#Testing on trained model\n",
        "def prediction(net: torch.nn.Module, eval_iter: DataLoader, device: torch.device):\n",
        "    net.eval()\n",
        "    loss_list=[]\n",
        "    U_list=[]\n",
        "    with torch.no_grad():\n",
        "        for X, y, z, _ in eval_iter:\n",
        "            X, y, z = X.to(device), y.to(device), z.to(device)\n",
        "            state=net.begin_state(batch_size=X.shape[0], device=device)\n",
        "            predP, output=net(X, state)\n",
        "            loss=lossFunc2(predP, y, output, z, device)\n",
        "            U, _=calcUtility(output, z)\n",
        "            loss_list.append(loss.cpu().numpy())\n",
        "            U_list.append(U[:, -1].mean().cpu().numpy())\n",
        "    return np.array(loss_list).mean(), np.array(U_list).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4zTVLtNGa6Yl"
      },
      "outputs": [],
      "source": [
        "#Trainer \n",
        "#Incoporated learning rate scheduler\n",
        "#Avg training loss & Avg validation Utility gain is recorded on epoch basis\n",
        "#Loss and Utility by epoch are plotted at the end of training\n",
        "def train(net: torch.nn.Module, train_iter: DataLoader, eval_iter: DataLoader, optimizer: torch.optim.Optimizer, device: torch.device, num_epoch: int, name: str):\n",
        "    loss_data=[]\n",
        "    U_data=[]\n",
        "    net.apply(init_weights)\n",
        "    net.to(device)\n",
        "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.95, last_epoch=-1)\n",
        "    \n",
        "    for epoch in range(num_epoch):\n",
        "        net.train()\n",
        "        lossEpoch, lossEpoch_list=train_epoch(net, train_iter, device, optimizer)   \n",
        "        loss_v, U_v=prediction(net, eval_iter, device)\n",
        "        loss_data.append(lossEpoch)  \n",
        "        U_data.append(U_v)\n",
        "        print(f'Epoch {epoch}, training loss: {lossEpoch:.2f}, val utility: {U_v:.2f}')\n",
        "        torch.save(net.state_dict(), os.path.join('../model_weights', f'{name}-epoch-{epoch}.pth'))\n",
        "        scheduler.step()\n",
        "    \n",
        "    #plot loss & Utility\n",
        "    fig, ax_left = plt.subplots()\n",
        "    ax_right = ax_left.twinx()\n",
        "    ax_left.plot(loss_data, label = \"Loss\", color='blue')\n",
        "    ax_right.plot(U_data, label = \"Utility\", color='red')\n",
        "    ax_left.set_xlabel('Time Step')\n",
        "    ax_right.set_ylabel('Loss y axis')\n",
        "    ax_right.set_ylabel('Utility y axis')\n",
        "    ax_left.set_title('Loss and Utility')\n",
        "    ax_left.legend()\n",
        "    ax_right.legend()\n",
        "    return loss_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qDQ2atIPa6Ym"
      },
      "outputs": [],
      "source": [
        "#Generation of training, validation, and testing dataset\n",
        "def DataIterGen(test_id_list, val_id_list, name_list, full_list, demo=False, bsize=32) -> tuple[DataLoader, DataLoader, DataLoader]:\n",
        "    \"\"\"\n",
        "    test_id_list: id of subjects for testing\n",
        "    val_id_list: id of subjects for validation\n",
        "    other subjects for training\n",
        "    full_list=get_data_set(name_list), preprocessed\n",
        "    demo: when demo mode is True, only test_iter is returned, with data from\n",
        "    first entry of test_id_list (single stock)\n",
        "    \"\"\"\n",
        "    name_count=len(name_list)\n",
        "\n",
        "    if demo:\n",
        "        test_iter=DataLoader(StockDataset(test_id_list[0:1], full_list, timestep=24, gap=1), shuffle=False, batch_size=64, num_workers=0)\n",
        "        print(f'Demo with stock: {name_list[test_id_list[0]]} ')\n",
        "        return test_iter\n",
        "    else:\n",
        "        train_list=set(name_list)-set([name_list[i] for i in test_id_list])-set([name_list[i] for i in val_id_list])\n",
        "        train_count=len(train_list)\n",
        "        partial_list=get_data_set(train_list)\n",
        "        test_iter=DataLoader(StockDataset(test_id_list, full_list), batch_size=bsize, num_workers=0)\n",
        "        val_iter=DataLoader(StockDataset(val_id_list, full_list), batch_size=bsize, num_workers=0)\n",
        "        train_iter=DataLoader(StockDataset(list(range(train_count)), partial_list), shuffle=True, batch_size=bsize, num_workers=0)\n",
        "        print(f'Val: {[name_list[val_id] for val_id in val_id_list]}, Test: {[name_list[test_id] for test_id in test_id_list]}, Train: {train_list} ')\n",
        "        return train_iter, val_iter, test_iter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NKzxSmYohHkA"
      },
      "outputs": [],
      "source": [
        "def test(net: torch.nn.Module, test_iter, device: torch.device, epoch, name):\n",
        "    net.eval()\n",
        "    net.load_state_dict(torch.load(os.path.join('../model_weights', f'{name}-epoch-{epoch}.pth')))\n",
        "    net.to(device)\n",
        "    U_list=[]\n",
        "    with torch.no_grad():\n",
        "        for X, _, _, zp in test_iter:\n",
        "            X, zp = X.to(device),  zp.to(device)\n",
        "            state=net.begin_state(batch_size=X.shape[0], device=device)\n",
        "            predP, output=net(X, state)\n",
        "            U, _=calcUtility(output, zp)\n",
        "            U_list.append(U[:, -1].mean().cpu().numpy())\n",
        "    return np.array(U_list).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xwNddm681EtG"
      },
      "outputs": [],
      "source": [
        "def demo(net: torch.nn.Module, demo_iter, device: torch.device, epoch, name):\n",
        "    net.eval()\n",
        "    net.load_state_dict(torch.load(os.path.join('../model_weights', f'{name}-epoch-{epoch}.pth')))\n",
        "    net.to(device)\n",
        "    reward=np.array([])\n",
        "    with torch.no_grad():\n",
        "        for X, _, _, zp in demo_iter:\n",
        "            X, zp = X.to(device),  zp.to(device)\n",
        "            state=net.begin_state(batch_size=X.shape[0], device=device)\n",
        "            predP, output=net(X, state)\n",
        "            discretizedAction=((output>=0)*2-1)\n",
        "            batchReward=discretizedAction*zp\n",
        "            reward=np.concatenate((reward,batchReward[:,-1].reshape(-1).cpu().numpy()))\n",
        "        result = [sum(reward[ : i + 1]) for i in range(len(reward))] \n",
        "    fig, ax_left = plt.subplots(figsize=(20,4))\n",
        "    ax_left.plot(result, label = \"Stock Gain\", color='blue')\n",
        "    ax_left.set_xlabel('Time Step')\n",
        "    ax_left.set_ylabel('Cumulative Gain')\n",
        "    ax_left.set_title('Demonstration of Algorithm')\n",
        "    ax_left.legend()\n",
        "    return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYnErDKxa6Ym"
      },
      "outputs": [],
      "source": [
        "#Data Generation\n",
        "\n",
        "stockList1=['COO','COF','ABBV','CCL','AMD','GOOG',\n",
        "            #'ABMD',\n",
        "            'ABT','ACN','ADBE','AES','NVDA',\n",
        "            'AIG','ALL', 'AMG','AMZN','APA','AAPL',\n",
        "            #'ATVI',\n",
        "            'AXP','BA','BBY','CAT','GE',\n",
        "            'CSCO', \n",
        "            #'DRE',\n",
        "            'EA','EQR','FCX','FE',\n",
        "            'HST','IBM','INTC','JCI','MMM','MO',\n",
        "            'ORCL','PPL','T','EXPD','VMC','VNO']\n",
        "if 'stock_dataset.npy' in os.listdir('../data'):\n",
        "    full_list = np.load(file='../data/stock_dataset.npy')\n",
        "    print(\"Loaded previous generation\")\n",
        "else:\n",
        "    print(\"Generating new dataset\")\n",
        "    full_list=get_data_set(stockList1, start=\"2017-01-01\", end=\"2020-01-01\", verbose=True, coint_threshold=0.1)\n",
        "    np.save(file='../data/stock_dataset.npy', arr=full_list)\n",
        "\n",
        "print(f\"Loaded dataset of {full_list.shape[0]} tickers, {full_list.shape[1]} ticks, {full_list.shape[2]} features/tick.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGTkYXk8uXRn",
        "outputId": "3599f841-a05b-4257-e0e7-dd64f6dc95dd"
      },
      "outputs": [],
      "source": [
        "train_iter, val_iter, test_iter=DataIterGen([0,1],[2,3,4], stockList1, full_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "B0MPDbw1a6Yn",
        "outputId": "6cc674ae-9fcb-47d3-a3dd-f73fe407c4a1"
      },
      "outputs": [],
      "source": [
        "#Training in action\n",
        "os.makedirs('../model_weights', exist_ok=True)\n",
        "\n",
        "net=rlPolicy(128, device)\n",
        "optim1 = optim.Adam(net.parameters(), lr=0.00006, weight_decay=0.01)\n",
        "loss=train(net, train_iter, val_iter, optim1, device, 14, 'DLRL')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lg5BLWG3BZa0",
        "outputId": "f3b8e4df-2e3c-4c96-8501-4684442bb798"
      },
      "outputs": [],
      "source": [
        "avgReturn=test(net, test_iter, device, 8, 'DLRL')\n",
        "print(f'On average, every 24 days the algorithm yields: {avgReturn}' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "OANMLYsFmGmS",
        "outputId": "90046830-49db-4d44-96e2-7d0a9765f546"
      },
      "outputs": [],
      "source": [
        "demo_iter=DataIterGen([1],[], stockList1,full_list, demo=True)\n",
        "demo(net, demo_iter , device, 9, 'DLRL')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "rNSspSTE_W26",
        "outputId": "5559bf5a-3627-450b-ace2-4f1b2d52316a"
      },
      "outputs": [],
      "source": [
        "demo_iter=DataIterGen([0],[], stockList1,full_list, demo=True)\n",
        "demo(net, demo_iter , device, 8, 'DLRL')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Project 2 with coint.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
